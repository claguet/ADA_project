{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping of votation outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "store = False # store CSV tables for all votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET GENERAL VOTATION INFO\n",
    "\n",
    "r = requests.get('https://www.admin.ch/ch/f/pore/va/vab_2_2_4_1_2011_2020.html')\n",
    "soup = BeautifulSoup(r.content, \"lxml\") #get the tree with beatifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET ALL VOTATION SESSIONS\n",
    "# Get all votation sessions and its url and store in dictionary votation_sessions\n",
    "\n",
    "votation_dates = soup.find_all('table')[0]\n",
    "votation_sessions = {}\n",
    "for dates in votation_dates.find_all('a', href=True):\n",
    "    # date : dates.text\n",
    "    # url : 'https://www.admin.ch/ch/f/pore/va/' + dates['href']\n",
    "    votation_sessions[dates.text] = ['https://www.admin.ch/ch/f/pore/va/' +  dates['href'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET INDIVIDUAL VOTATIONS FOR EACH SESSION\n",
    "\n",
    "votations_code = [] # list with codes of all votations\n",
    "\n",
    "for date in votation_sessions: # loop over all votation sessions\n",
    "    url = votation_sessions[date]\n",
    "    url_session = url[0][:-11]\n",
    "    \n",
    "    # Get is html content\n",
    "    r = requests.get(url[0])\n",
    "    soup_date = BeautifulSoup(r.content, \"lxml\")\n",
    "    my_soup = soup_date.find_all('p')\n",
    "    \n",
    "    # Get individual votation codes for a given session\n",
    "    votation_code = []\n",
    "    for a in my_soup:\n",
    "        url_list = a.find_all('a', href=True)\n",
    "        if url_list != []:\n",
    "            for url in url_list:\n",
    "                my_url = url['href']\n",
    "                if my_url[2] == 'c': \n",
    "                    votation_code.append(url_session + my_url[1:])\n",
    "                    votations_code.append(my_url[5:-5])\n",
    "                    \n",
    "    # Append votation codes to dictionary votation_sessions together with the votation date and url\n",
    "    votation_sessions[date].append(votation_code)\n",
    "    \n",
    "num_votations = len(votations_code) # counter that gives you the total number of votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrapeTable(info):\n",
    "    \"\"\" Create a pandas DataFrame with the html table\n",
    "    info: beautiful soup file\n",
    "    table: pandas data frame with scraped info\"\"\"\n",
    "    \n",
    "    # Define the names of the data frame fields\n",
    "    field_names = []\n",
    "    for field in info.find(\"thead\").find_all('td'): # the first tr contains the field names\n",
    "        field_names.append(field.text)\n",
    "    table = pd.DataFrame(columns = field_names)\n",
    "\n",
    "    # Populate data frame with data extracted from html table\n",
    "    for index, col_label in enumerate(table.columns): # fill each data frame columns\n",
    "        # in the html file, look for all data that correspond to that given column (Hint: they are always separated by len(table.columns))\n",
    "        col_data = [data.text for i, data in enumerate(info.find(\"tbody\").find_all('td')) if (i%len(table.columns) == index)]\n",
    "        # fill the data frame\n",
    "        table[col_label] = col_data\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def storeCSVfile(date,all_tables):\n",
    "    \"\"\"Create tables in CSV format from data frame corresponding to the input date\"\"\"\n",
    "    votation_names_text.write(date)\n",
    "    votation_names_text.write('\\n')\n",
    "    \n",
    "    # Create a folder for the given date\n",
    "    date_short = [i for i in date if i !='.']\n",
    "    date_short = ''.join(date_short)\n",
    "    newpath = \"Votation\" + \"\\\\\" + date_short \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "    session_data = votation_sessions[date]\n",
    "    bug_chars = ['’',' ' ,'\\n','(',')' ,\"'\" ,\".\" ,\",\",'«','»','-']\n",
    "\n",
    "    for votation in session_data[1]: # For each votation in the session\n",
    "        session_code =votation[-11:-5]\n",
    "        \n",
    "        # Scrape the data from url\n",
    "        session_r = requests.get(votation)\n",
    "        session_soup = BeautifulSoup(session_r.content, \"lxml\")\n",
    "        session_info = session_soup.find_all('table')[0]\n",
    "        session_table = scrapeTable(session_info)\n",
    "        votation_name = session_soup.find_all('h3')[0].text\n",
    "        votation_name_df = pd.DataFrame(data = {'Votation Title': [votation_name]})\n",
    "        \n",
    "        votation_names_text.write(votation_name)\n",
    "        votation_names_text.write('\\n')\n",
    "        \n",
    "        # Define path for the given table\n",
    "        votation_path = newpath + '\\\\' + session_code + '.xlsx'\n",
    "        \n",
    "        #all_tables.append(session_table)\n",
    "        all_tables[int(session_code[3:])] = session_table\n",
    "        \n",
    "        # Store the data frame in CSV file in newpath location\n",
    "        if store: \n",
    "            writer = pd.ExcelWriter(votation_path)\n",
    "            session_table.to_excel(writer,sheet_name = 'Sheet1') # table is stored in Sheet 1\n",
    "            votation_name_df.to_excel(writer,sheet_name = 'Sheet2') # votation name and description is stored in Sheet 2\n",
    "            writer.save()\n",
    "            \n",
    "    return all_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build large dataframe with all votation tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "votation_names_text = open('votation_names.txt', 'w')\n",
    "all_tables = {}\n",
    "for date in votation_sessions:\n",
    "    all_tables = storeCSVfile(date,all_tables)\n",
    "votation_names_text.close()\n",
    "# Note: we write the names of the votations in a text file for manual inspection and votation type definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_columns = ['Canton', 'Electeurs', 'Votants', '% Particip.', 'Oui', 'Non', '% Oui', '% Non']\n",
    "\n",
    "pds = []\n",
    "for votation_code in all_tables:\n",
    "    table = all_tables[votation_code]\n",
    "    cols = pd.MultiIndex.from_product([votation_code, sub_columns])\n",
    "    pds.append(pd.DataFrame(table.as_matrix(), columns=cols))\n",
    "\n",
    "result = pd.concat(pds, axis=1)\n",
    "result.head()\n",
    "\n",
    "writer = pd.ExcelWriter('Votation\\\\all_votations.xlsx')\n",
    "result.to_excel(writer,sheet_name = 'Sheet1') # table is stored in Sheet 1\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign votation catergories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign votations to major votation topics (manually)\n",
    "votation_subject = {'travail':[557,568,574,575,583,601],'environnement':[555, 556,566,569,577,578,588,591,595,599,602],'economie':[585,587,589,594,598],'immigration':[561,571,580,597,604],'education':[559,563,593],'securite':[554,572,582,584],'sante':[562,565,573,579,581,586,592,603],'social':[558,560,564,567,570,576,590,596,600]}\n",
    "\n",
    "#       travail:  work, holidays\n",
    "# environnement:  environment, agriculture, transport, energy\n",
    "#   immigration:  foreigners affairs\n",
    "#      securite:  police, military, justice\n",
    "#         sante:  health\n",
    "#        social:  family, retirement, elderly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'economie': [585, 587, 589, 594, 598],\n",
       " 'education': [559, 563, 593],\n",
       " 'environnement': [555, 556, 566, 569, 577, 578, 588, 591, 595, 599, 602],\n",
       " 'immigration': [561, 571, 580, 597, 604],\n",
       " 'sante': [562, 565, 573, 579, 581, 586, 592, 603],\n",
       " 'securite': [554, 572, 582, 584],\n",
       " 'social': [558, 560, 564, 567, 570, 576, 590, 596, 600],\n",
       " 'travail': [557, 568, 574, 575, 583, 601]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votation_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "pickle.dump(votation_subject,open( \"votations_sorted.p\", \"wb\" ))\n",
    "\n",
    "# Load pickle\n",
    "# votations_sorted = pickle.load( open( \"votations_sorted.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
